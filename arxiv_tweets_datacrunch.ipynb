{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Crunching the data\n",
    "\n",
    "This code processes the raw data gathered with arxiv_tweets_get_data.ipynb . Result is a table indexed by id's of the papers that contains the count of tweets for every paper, link to the paper, it's title, scientific area and times of tweets about the paper.\n",
    "\n",
    "Title and scientific area of the paper are obtained by accesing Arxiv API.\n",
    "\n",
    "Resulting data is saved as data.json ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os.path as op\n",
    "\n",
    "#to acces Arxiv API and to parse its reply\n",
    "import urllib2 as ul\n",
    "import xml.etree.ElementTree as et\n",
    "\n",
    "#load data\n",
    "raw_data=pd.read_csv( op.join('data','raw_data.csv'),\n",
    "                     names=['Arxiv_Id','Version','Time'],\n",
    "                     dtype={'Arxiv_Id':str,'Version':int,'Time':pd.datetime})\n",
    "\n",
    "d=pd.DataFrame(columns=['Title','Category','Area','Version','Link','Tweets','Times'])\n",
    "\n",
    "#count tweets for every paper\n",
    "d.Tweets=raw_data.Arxiv_Id.value_counts()\n",
    "\n",
    "#find the highest version of the paper that occures in raw_data\n",
    "d.Version=d.index.map(lambda x: raw_data[raw_data.Arxiv_Id==x]['Version'].max())\n",
    "\n",
    "#construct links\n",
    "d.Link='https://arxiv.org/abs/'+d.index.where(d.Version==0, d.index+'v'+d.Version.map(str))\n",
    "\n",
    "#get the times\n",
    "d.Times=d.index.map(lambda x: raw_data[raw_data.Arxiv_Id==x].Time.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#translation of category into area\n",
    "cat_dict={'cond-mat': 'Physics',\n",
    "          'math': 'Math',\n",
    "          'cs': 'Computer Science',\n",
    "          'q-bio': 'Biology',\n",
    "          'stat' : 'Statistics',\n",
    "          'astro-ph': 'Physics',\n",
    "          'gr-qc': 'Physics',\n",
    "          'hep-ex': 'Physics',\n",
    "          'hep-lat': 'Physics',\n",
    "          'hep-ph': 'Physics',\n",
    "          'hep-th': 'Physics',\n",
    "          'math-ph': 'Physics',\n",
    "          'nucl-ex': 'Physics',\n",
    "          'nucl-th': 'Physics',\n",
    "          'physics': 'Physics',\n",
    "          'quant-ph': 'Physics',\n",
    "          'q-fin': 'Quantitative Finance',\n",
    "          'nlin': 'Physics'\n",
    "         }\n",
    "\n",
    "#connects to Arxiv API, returns paper's title, category and area\n",
    "#WARNING: may take some time for big tables- a lot of requests to Arxiv\n",
    "\n",
    "def get_arxiv_info(link):\n",
    "    querry='http://export.arxiv.org/api/query?id_list='+link.split('/')[-1]\n",
    "    print 'Accesing: '+querry\n",
    "    try:\n",
    "        data = ul.urlopen(querry).read()    #get data via Arxiv API\n",
    "    except Exception,e:\n",
    "        print repr(e)\n",
    "        return (None,None,None)\n",
    "        \n",
    "    root=et.fromstring(data)    #parse the data\n",
    "    for elem in root.iter():\n",
    "        if elem.tag=='{http://www.w3.org/2005/Atom}entry':\n",
    "            entryelem=elem\n",
    "            break\n",
    "            \n",
    "    category=None\n",
    "    title=None\n",
    "    for elem in entryelem.iter():\n",
    "        if elem.tag=='{http://www.w3.org/2005/Atom}title':\n",
    "            title= elem.text.replace('\\n ','').replace('\\n','')\n",
    "        if 'category' in elem.tag and category==None:\n",
    "            category=elem.attrib['term']\n",
    "    \n",
    "    try:\n",
    "        area=cat_dict[category.split('.')[0]]    #translate the category\n",
    "    except KeyError:\n",
    "        area=None\n",
    "    return pd.Series([title,area,category])\n",
    "\n",
    "print 'There are '+str(len(d.index))+' querries to be made.'\n",
    "d[['Title','Area','Category']]=d.Link.apply(get_arxiv_info)\n",
    "\n",
    "d.to_json(op.join('data','data.json'))\n",
    "print \"\\nFinished, final data saved as 'data/data.json' \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
